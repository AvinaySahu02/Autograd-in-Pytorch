# Autograd-in-Pytorch

⚙️ Autograd and Gradient Computation in PyTorch

This project dives into PyTorch’s Autograd system, the automatic differentiation engine that enables backpropagation and model optimization.
Through practical examples, it demonstrates how gradients are calculated, tracked and applied to update model parameters — a crucial step in training deep learning models.

🧩 What You’ll Learn

1.  Autograd builds and manages dynamic computation graphs

2. Using requires_grad to track operations on tensors

3. Computing gradients with the backward() method

4. Accessing and interpreting auto-grad attributes

6. Performing manual gradient checks for learning purposes

7. Temporarily disabling gradient tracking with torch.no_grad()

8. Understanding how gradient flow drives learning in neural networks

🎓 Learning Outcome

By the end of this module, you’ll have a clear understanding of how PyTorch automatically handles differentiation, enabling smooth and efficient training of neural networks.

🧠 Tech Overview

Language: Python

Framework: PyTorch

Focus Area: Deep Learning Fundamentals
